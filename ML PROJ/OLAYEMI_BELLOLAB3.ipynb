{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93e70321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas \n",
    "# Read the CSV file\n",
    "bankrupt = pandas.read_csv(\"bankrupt.txt\", sep=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4ce3933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: C:\\Users\\User\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09f55068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Observations in the dataset: 6819\n",
      "Nmber of Variables in the dataset: 96\n"
     ]
    }
   ],
   "source": [
    "# Using the shape attribute to find the number of observations and variables\n",
    "num_rows, num_columns = bankrupt.shape\n",
    "\n",
    "print(\"Number of Observations in the dataset:\",num_rows)\n",
    "print(\"Nmber of Variables in the dataset:\", num_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382d77d3-1706-4d24-95c7-af6f1db0accf",
   "metadata": {},
   "source": [
    "# 2.1.2 Graded Question.\n",
    "How many observations and variables are there? \n",
    "There are 6819 Observations and 96 Variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8d15102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Descriptive  Statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bankrupt?</th>\n",
       "      <th>ROA(C) before interest and depreciation before interest</th>\n",
       "      <th>ROA(A) before interest and % after tax</th>\n",
       "      <th>ROA(B) before interest and depreciation after tax</th>\n",
       "      <th>Operating Gross Margin</th>\n",
       "      <th>Realized Sales Gross Margin</th>\n",
       "      <th>Operating Profit Rate</th>\n",
       "      <th>Pre-tax net Interest Rate</th>\n",
       "      <th>After-tax net Interest Rate</th>\n",
       "      <th>Non-industry income and expenditure/revenue</th>\n",
       "      <th>...</th>\n",
       "      <th>Net Income to Total Assets</th>\n",
       "      <th>Total assets to GNP price</th>\n",
       "      <th>No-credit Interval</th>\n",
       "      <th>Gross Profit to Sales</th>\n",
       "      <th>Net Income to Stockholder s Equity</th>\n",
       "      <th>Liability to Equity</th>\n",
       "      <th>Degree of Financial Leverage (DFL)</th>\n",
       "      <th>Interest Coverage Ratio (Interest expense to EBIT)</th>\n",
       "      <th>Net Income Flag</th>\n",
       "      <th>Equity to Liability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6819.000000</td>\n",
       "      <td>6819.000000</td>\n",
       "      <td>6819.000000</td>\n",
       "      <td>6819.000000</td>\n",
       "      <td>6819.000000</td>\n",
       "      <td>6819.000000</td>\n",
       "      <td>6819.000000</td>\n",
       "      <td>6819.000000</td>\n",
       "      <td>6819.000000</td>\n",
       "      <td>6819.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6819.000000</td>\n",
       "      <td>6.819000e+03</td>\n",
       "      <td>6819.000000</td>\n",
       "      <td>6819.000000</td>\n",
       "      <td>6819.000000</td>\n",
       "      <td>6819.000000</td>\n",
       "      <td>6819.000000</td>\n",
       "      <td>6819.000000</td>\n",
       "      <td>6819.0</td>\n",
       "      <td>6819.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.032263</td>\n",
       "      <td>0.505180</td>\n",
       "      <td>0.558625</td>\n",
       "      <td>0.553589</td>\n",
       "      <td>0.607948</td>\n",
       "      <td>0.607929</td>\n",
       "      <td>0.998755</td>\n",
       "      <td>0.797190</td>\n",
       "      <td>0.809084</td>\n",
       "      <td>0.303623</td>\n",
       "      <td>...</td>\n",
       "      <td>0.807760</td>\n",
       "      <td>1.862942e+07</td>\n",
       "      <td>0.623915</td>\n",
       "      <td>0.607946</td>\n",
       "      <td>0.840402</td>\n",
       "      <td>0.280365</td>\n",
       "      <td>0.027541</td>\n",
       "      <td>0.565358</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.047578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.176710</td>\n",
       "      <td>0.060686</td>\n",
       "      <td>0.065620</td>\n",
       "      <td>0.061595</td>\n",
       "      <td>0.016934</td>\n",
       "      <td>0.016916</td>\n",
       "      <td>0.013010</td>\n",
       "      <td>0.012869</td>\n",
       "      <td>0.013601</td>\n",
       "      <td>0.011163</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040332</td>\n",
       "      <td>3.764501e+08</td>\n",
       "      <td>0.012290</td>\n",
       "      <td>0.016934</td>\n",
       "      <td>0.014523</td>\n",
       "      <td>0.014463</td>\n",
       "      <td>0.015668</td>\n",
       "      <td>0.013214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.476527</td>\n",
       "      <td>0.535543</td>\n",
       "      <td>0.527277</td>\n",
       "      <td>0.600445</td>\n",
       "      <td>0.600434</td>\n",
       "      <td>0.998969</td>\n",
       "      <td>0.797386</td>\n",
       "      <td>0.809312</td>\n",
       "      <td>0.303466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.796750</td>\n",
       "      <td>9.036205e-04</td>\n",
       "      <td>0.623636</td>\n",
       "      <td>0.600443</td>\n",
       "      <td>0.840115</td>\n",
       "      <td>0.276944</td>\n",
       "      <td>0.026791</td>\n",
       "      <td>0.565158</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.024477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.502706</td>\n",
       "      <td>0.559802</td>\n",
       "      <td>0.552278</td>\n",
       "      <td>0.605997</td>\n",
       "      <td>0.605976</td>\n",
       "      <td>0.999022</td>\n",
       "      <td>0.797464</td>\n",
       "      <td>0.809375</td>\n",
       "      <td>0.303525</td>\n",
       "      <td>...</td>\n",
       "      <td>0.810619</td>\n",
       "      <td>2.085213e-03</td>\n",
       "      <td>0.623879</td>\n",
       "      <td>0.605998</td>\n",
       "      <td>0.841179</td>\n",
       "      <td>0.278778</td>\n",
       "      <td>0.026808</td>\n",
       "      <td>0.565252</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.033798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.535563</td>\n",
       "      <td>0.589157</td>\n",
       "      <td>0.584105</td>\n",
       "      <td>0.613914</td>\n",
       "      <td>0.613842</td>\n",
       "      <td>0.999095</td>\n",
       "      <td>0.797579</td>\n",
       "      <td>0.809469</td>\n",
       "      <td>0.303585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.826455</td>\n",
       "      <td>5.269777e-03</td>\n",
       "      <td>0.624168</td>\n",
       "      <td>0.613913</td>\n",
       "      <td>0.842357</td>\n",
       "      <td>0.281449</td>\n",
       "      <td>0.026913</td>\n",
       "      <td>0.565725</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.052838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.820000e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Bankrupt?   ROA(C) before interest and depreciation before interest  \\\n",
       "count  6819.000000                                        6819.000000          \n",
       "mean      0.032263                                           0.505180          \n",
       "std       0.176710                                           0.060686          \n",
       "min       0.000000                                           0.000000          \n",
       "25%       0.000000                                           0.476527          \n",
       "50%       0.000000                                           0.502706          \n",
       "75%       0.000000                                           0.535563          \n",
       "max       1.000000                                           1.000000          \n",
       "\n",
       "        ROA(A) before interest and % after tax  \\\n",
       "count                              6819.000000   \n",
       "mean                                  0.558625   \n",
       "std                                   0.065620   \n",
       "min                                   0.000000   \n",
       "25%                                   0.535543   \n",
       "50%                                   0.559802   \n",
       "75%                                   0.589157   \n",
       "max                                   1.000000   \n",
       "\n",
       "        ROA(B) before interest and depreciation after tax  \\\n",
       "count                                        6819.000000    \n",
       "mean                                            0.553589    \n",
       "std                                             0.061595    \n",
       "min                                             0.000000    \n",
       "25%                                             0.527277    \n",
       "50%                                             0.552278    \n",
       "75%                                             0.584105    \n",
       "max                                             1.000000    \n",
       "\n",
       "        Operating Gross Margin   Realized Sales Gross Margin  \\\n",
       "count              6819.000000                   6819.000000   \n",
       "mean                  0.607948                      0.607929   \n",
       "std                   0.016934                      0.016916   \n",
       "min                   0.000000                      0.000000   \n",
       "25%                   0.600445                      0.600434   \n",
       "50%                   0.605997                      0.605976   \n",
       "75%                   0.613914                      0.613842   \n",
       "max                   1.000000                      1.000000   \n",
       "\n",
       "        Operating Profit Rate   Pre-tax net Interest Rate  \\\n",
       "count             6819.000000                 6819.000000   \n",
       "mean                 0.998755                    0.797190   \n",
       "std                  0.013010                    0.012869   \n",
       "min                  0.000000                    0.000000   \n",
       "25%                  0.998969                    0.797386   \n",
       "50%                  0.999022                    0.797464   \n",
       "75%                  0.999095                    0.797579   \n",
       "max                  1.000000                    1.000000   \n",
       "\n",
       "        After-tax net Interest Rate  \\\n",
       "count                   6819.000000   \n",
       "mean                       0.809084   \n",
       "std                        0.013601   \n",
       "min                        0.000000   \n",
       "25%                        0.809312   \n",
       "50%                        0.809375   \n",
       "75%                        0.809469   \n",
       "max                        1.000000   \n",
       "\n",
       "        Non-industry income and expenditure/revenue  ...  \\\n",
       "count                                   6819.000000  ...   \n",
       "mean                                       0.303623  ...   \n",
       "std                                        0.011163  ...   \n",
       "min                                        0.000000  ...   \n",
       "25%                                        0.303466  ...   \n",
       "50%                                        0.303525  ...   \n",
       "75%                                        0.303585  ...   \n",
       "max                                        1.000000  ...   \n",
       "\n",
       "        Net Income to Total Assets   Total assets to GNP price  \\\n",
       "count                  6819.000000                6.819000e+03   \n",
       "mean                      0.807760                1.862942e+07   \n",
       "std                       0.040332                3.764501e+08   \n",
       "min                       0.000000                0.000000e+00   \n",
       "25%                       0.796750                9.036205e-04   \n",
       "50%                       0.810619                2.085213e-03   \n",
       "75%                       0.826455                5.269777e-03   \n",
       "max                       1.000000                9.820000e+09   \n",
       "\n",
       "        No-credit Interval   Gross Profit to Sales  \\\n",
       "count          6819.000000             6819.000000   \n",
       "mean              0.623915                0.607946   \n",
       "std               0.012290                0.016934   \n",
       "min               0.000000                0.000000   \n",
       "25%               0.623636                0.600443   \n",
       "50%               0.623879                0.605998   \n",
       "75%               0.624168                0.613913   \n",
       "max               1.000000                1.000000   \n",
       "\n",
       "        Net Income to Stockholder s Equity   Liability to Equity  \\\n",
       "count                          6819.000000           6819.000000   \n",
       "mean                              0.840402              0.280365   \n",
       "std                               0.014523              0.014463   \n",
       "min                               0.000000              0.000000   \n",
       "25%                               0.840115              0.276944   \n",
       "50%                               0.841179              0.278778   \n",
       "75%                               0.842357              0.281449   \n",
       "max                               1.000000              1.000000   \n",
       "\n",
       "        Degree of Financial Leverage (DFL)  \\\n",
       "count                          6819.000000   \n",
       "mean                              0.027541   \n",
       "std                               0.015668   \n",
       "min                               0.000000   \n",
       "25%                               0.026791   \n",
       "50%                               0.026808   \n",
       "75%                               0.026913   \n",
       "max                               1.000000   \n",
       "\n",
       "        Interest Coverage Ratio (Interest expense to EBIT)   Net Income Flag  \\\n",
       "count                                        6819.000000              6819.0   \n",
       "mean                                            0.565358                 1.0   \n",
       "std                                             0.013214                 0.0   \n",
       "min                                             0.000000                 1.0   \n",
       "25%                                             0.565158                 1.0   \n",
       "50%                                             0.565252                 1.0   \n",
       "75%                                             0.565725                 1.0   \n",
       "max                                             1.000000                 1.0   \n",
       "\n",
       "        Equity to Liability  \n",
       "count           6819.000000  \n",
       "mean               0.047578  \n",
       "std                0.050014  \n",
       "min                0.000000  \n",
       "25%                0.024477  \n",
       "50%                0.033798  \n",
       "75%                0.052838  \n",
       "max                1.000000  \n",
       "\n",
       "[8 rows x 96 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate descriptive statistics using describe()\n",
    "print(\"\\nDescriptive  Statistics:\")\n",
    "bankrupt.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db604fb7-2e81-4677-b0d4-b8dabe78fd38",
   "metadata": {},
   "source": [
    "# 2.1.3 Graded Question.\n",
    "Comments on the results.\n",
    "\n",
    "For each variable (input/feature) there are 6819 observations which are depicted above in the count row.\n",
    "The mean of the observations per variable in the dataset is shown next, being the weighted average obtained. \n",
    "The standard deviation (std) shows how dispersed the data is from the mean and it is very instrumental in telling us about the dataset. \n",
    "\n",
    "It is observed that there is low deviation from the mean across some features except \"Total assets to GNP price\" which is very high followed by ROA(C), ROA(B) ROA(A), \"Net Income to Total Asset\" which indicates a broader or dispersed distribution within these features.\n",
    "\n",
    "Additionally, the minimum value of each observation for each variable (i.e. in each column is shown on the min row likewise the maximum value is in the max row (the last row in the description (i.e. max).\n",
    "\n",
    "The 25% represents the 25th percentile of each column, that is, the value at which 25% of the values in this column lie below. \n",
    "The 50% represents the 50th percentile which is the median, which cuts the data in each column of the dataset into half. \n",
    "The 75% presents the 75th percentile, which means 75% of the values lie above this particular value. \n",
    "For example, using the “ROA(C)” variable column, the 75% is 0.535563, which means ¾ of the values in this column lie above 0.535563.\n",
    "\n",
    "    \n",
    "Do you think it is necessary to standardize the variables before performing classi\f",
    "cation?\n",
    "Based on the statistics obtained in this case, the features have different scales, and the standard deviations vary. For example, \"ROA(C) before interest and depreciation before interest\" has a much smaller standard deviation compared to \"Total assets to GNP price.\" This indicates that the features have varying scales.\n",
    "\n",
    "Examining the \"min\" and \"max\" values for each feature, Some features, like \"Total assets to GNP price\" have a wide range of values, including potential outliers that could affect the performance of some classification.\n",
    "\n",
    "Features like \"Net Income to Total Asset\", \"Total assets to GNP price\" and \"Operating Profit Rate\" have different mean and standard deviation values, indicating potentially different distributions.\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "Based on these observations, it is concluded that the features in the dataset have varying scales, and there are features with a wide range of values, and different distributions (mean, and standard deviation). Therefore, it is necessary to standardize the variables before performing classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "160b5053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_balanced: False\n",
      "Number of companies that went bankrupt: 220\n",
      "Percentage of companies that went bankrupt: 3.23%\n",
      "Is the dataset balanced? No\n"
     ]
    }
   ],
   "source": [
    "# establish the target variable is named 'Bankrupt?'\n",
    "target_variable = 'Bankrupt?'\n",
    "\n",
    "# Use value_counts() to count the unique values in the target variable\n",
    "value_counts = bankrupt[target_variable].value_counts()\n",
    "\n",
    "#print(value_counts)\n",
    "\n",
    "# Get the number of companies that went bankrupt (having that 'Bankrupt' is encoded as 1)\n",
    "num_bankrupt = value_counts.get(1, 0)  # Get the count of 1's (companies that went bankrupt)\n",
    "\n",
    "# Calculate the percentage of companies that went bankrupt\n",
    "percentage_bankrupt = (num_bankrupt / len(bankrupt)) * 100\n",
    "\n",
    "# Check if the dataset is balanced\n",
    "is_balanced = value_counts.min() / value_counts.max() > 0.5  # Adjust the threshold as needed\n",
    "\n",
    "# Print the results\n",
    "print(f\"is_balanced: {is_balanced}\")\n",
    "print(f\"Number of companies that went bankrupt: {num_bankrupt}\")\n",
    "print(f\"Percentage of companies that went bankrupt: {percentage_bankrupt:.2f}%\")\n",
    "print(f\"Is the dataset balanced? {'Yes' if is_balanced else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ebce77-3cb3-4764-8311-5102dc1918ab",
   "metadata": {},
   "source": [
    "# 2.1.4 [Graded question] \n",
    "\n",
    "Describe the target variable.\n",
    "The target variable (Y) is a qualitative variable with two classes predicting whether a company will be bankrupt or not.\n",
    "\n",
    "What is the number and percentage of companies that went bankrupt? \n",
    "3.23%\n",
    "\n",
    "Is the data set balanced? No, it is not balanced based on the findings in answer to Question 2.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "137d365a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Observations in x Train Data: 5455\n",
      "Number of Observations in x Test Data: 1364\n",
      "Number of Observations in y Train Data: 5455\n",
      "Number of Observations in y Test Data: 1364\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "# Import the train and test datasets\n",
    "x_train = pd.read_csv(\"x_train.csv\", index_col=0)\n",
    "x_test = pd.read_csv(\"x_test.csv\", index_col=0)\n",
    "y_train = pd.read_csv(\"y_train.csv\", index_col=0)\n",
    "y_test = pd.read_csv(\"y_test.csv\", index_col=0)\n",
    "\n",
    "# Display the number of observations in each dataset\n",
    "num_observations_train_x = x_train.shape[0]\n",
    "num_observations_test_x = x_test.shape[0]\n",
    "num_observations_train_y = y_train.shape[0]\n",
    "num_observations_test_y = y_test.shape[0]\n",
    "\n",
    "print(\"Number of Observations in x Train Data:\", num_observations_train_x)\n",
    "print(\"Number of Observations in x Test Data:\", num_observations_test_x)\n",
    "print(\"Number of Observations in y Train Data:\", num_observations_train_y)\n",
    "print(\"Number of Observations in y Test Data:\", num_observations_test_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251b4057-3591-4b81-8c58-561186255fd2",
   "metadata": {},
   "source": [
    "# 2.1.5A [graded question] \n",
    "\n",
    "How many observations are in each data set? \n",
    "Number of Observations in x Train Data: 5455\r\n",
    "Number of Observations in x Test Data: 1364\r\n",
    "Number of Observations in y Train Data: 5455\r\n",
    "Number of Observations in y Test Data: 1ts ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "554d2de6-52a7-49d4-87fe-3737f8fab4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribution of Classes in y train Data:\n",
      "Bankrupt\n",
      "0    5281\n",
      "1     174\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribution of Classes in y test Data:\n",
      "Bankrupt\n",
      "0    1318\n",
      "1      46\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the distribution of classes in the target variable in both datasets\n",
    "target_variable_train = \"Bankrupt\"\n",
    "target_variable_test = \"Bankrupt\"\n",
    "\n",
    "bankrupt_counts_train = y_train[target_variable_train].value_counts()\n",
    "bankrupt_counts_test = y_test[target_variable_test].value_counts()\n",
    "\n",
    "print(\"\\nDistribution of Classes in y train Data:\")\n",
    "print(bankrupt_counts_train)\n",
    "\n",
    "print(\"\\nDistribution of Classes in y test Data:\")\n",
    "print(bankrupt_counts_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "862296e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Is the Distribution of Classes Similar in Both Data Sets? False\n",
      "\n",
      "The propotion of classes in y train is: Bankrupt\n",
      "0    0.968103\n",
      "1    0.031897\n",
      "Name: count, dtype: float64\n",
      "\n",
      "The propotion of classes in y test is: Bankrupt\n",
      "0    0.966276\n",
      "1    0.033724\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate proportions in both datasets\n",
    "proportions_train = bankrupt_counts_train / num_observations_train_y\n",
    "proportions_test = bankrupt_counts_test / num_observations_test_y\n",
    "\n",
    "# Check if the distribution is similar\n",
    "is_distribution_similar = proportions_train.equals(proportions_test)\n",
    "\n",
    "print(\"\\nIs the Distribution of Classes Similar in Both Data Sets?\", is_distribution_similar)\n",
    "\n",
    "#ADDED CODE\n",
    "print(\"\\nThe propotion of classes in y train is:\",proportions_train)\n",
    "print(\"\\nThe propotion of classes in y test is:\",proportions_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28394830-463e-476f-95f6-3a419977315f",
   "metadata": {},
   "source": [
    "# 2.1.5B [Graded Question] \n",
    "\n",
    "Is the distribution of the classes of the target variable similar in both data sets?\n",
    "\n",
    "No the distribution of the classes are not the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9fb2013-4d9e-4327-ae51-ecba7b0cee4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDITED\n",
    "\n",
    "#import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "248c50c3-05f0-411c-8190-2fd6e67276eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate x_train and y_train\n",
    "bankrupt_train = pd.concat([x_train, y_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3281161-2a15-4132-9222-55bf216bc9e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5455, 13)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bankrupt_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "711254fe-cd61-42d3-8b16-b4b066b1d01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = scaler_x.fit_transform(x_train)\n",
    "y_train = scaler_y.fit_transform(y_train)\n",
    "\n",
    "x_test = scaler_x.transform(x_test)\n",
    "y_test = scaler_y.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8758198a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:               Bankrupt   No. Observations:                 5455\n",
      "Model:                            GLM   Df Residuals:                     5443\n",
      "Model Family:                Binomial   Df Model:                           11\n",
      "Link Function:                  Logit   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -513.79\n",
      "Date:                Sun, 15 Oct 2023   Deviance:                       1027.6\n",
      "Time:                        23:52:59   Pearson chi2:                 4.44e+06\n",
      "No. Iterations:                    10   Pseudo R-squ. (CS):            0.08988\n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      6.0644      3.655      1.659      0.097      -1.099      13.228\n",
      "ROAC         -11.3247      7.788     -1.454      0.146     -26.589       3.940\n",
      "ROAA          -1.5478      6.154     -0.252      0.801     -13.609      10.513\n",
      "ROAB          -3.5475      8.761     -0.405      0.686     -20.718      13.623\n",
      "TRA           -1.7574      0.973     -1.807      0.071      -3.664       0.149\n",
      "TAGR        3.738e-11   3.79e-11      0.985      0.324    -3.7e-11    1.12e-10\n",
      "DR            20.5210      2.743      7.480      0.000      15.144      25.898\n",
      "WKTA          -4.9171      2.429     -2.024      0.043      -9.678      -0.156\n",
      "CTA           -6.7206      1.708     -3.934      0.000     -10.068      -3.373\n",
      "CLA           -6.6835      2.378     -2.810      0.005     -11.345      -2.022\n",
      "CFOA           2.7174      1.772      1.534      0.125      -0.755       6.190\n",
      "CLCA           0.5781      2.178      0.265      0.791      -3.690       4.847\n",
      "NITA          -1.3517      6.213     -0.218      0.828     -13.529      10.825\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "##import the required modules and classes for logistic regression\n",
    "#Logistic Regression\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "\n",
    "# Define the logistic regression model\n",
    "model_formula = \"Bankrupt ~ ROAC + ROAA + ROAB + TRA + TAGR + DR + WKTA + CTA + CLA + CFOA + CLCA + NITA\"\n",
    "\n",
    "# Fit logistic regression model\n",
    "logit_model = smf.glm(formula=model_formula, data=bankrupt_train, family=sm.families.Binomial())\n",
    "\n",
    "logreg = logit_model.fit()\n",
    "\n",
    "# Display the summary of the logistic regression model\n",
    "print(logreg.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd03c300-bef4-4113-9373-d43e2f78492a",
   "metadata": {},
   "source": [
    "# 2.2.1. [Graded question]\n",
    "\n",
    "Do any of the predictors appear to be statistically significant? \n",
    "The lower p-values indicate higher statistical significance. Assume that the threshold p-values is 0.05. Therefore, the predictors with p-values less than 0.05 are considered to be statistically significant.\n",
    "\n",
    "If so, which ones?\n",
    "DR, WKTA, and CTA are statistically significant predictors.\n",
    "\n",
    "Interpret the \n",
    "coffi\u000ecient estimates of thesignificant  feature.\n",
    "\n",
    "For a significant predictor DR, its coefficient is positive and it indicates a positive relationship with the target variable (Bankrupt), while the coefficients of the predictors WKTA and CTA are negative and suggest a negative relationship with the target variable. \n",
    "The magnitude of the coefficient of predictor DR also indicates the strength of its relationship with the target variable compared with the other two significant predictors.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "509611a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[5266  143]\n",
      " [  15   31]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.997     0.974     0.985      5409\n",
      "           1      0.178     0.674     0.282        46\n",
      "\n",
      "    accuracy                          0.971      5455\n",
      "   macro avg      0.588     0.824     0.634      5455\n",
      "weighted avg      0.990     0.971     0.979      5455\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#EDITED ADDED\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Get the estimated probabilities\n",
    "yhat_logreg_probs = logreg.fittedvalues\n",
    "\n",
    "# Convert probabilities to binary class labels using a threshold of 0.5\n",
    "yhat = [1 if x > 0.5 else 0 for x in yhat_logreg_probs]\n",
    "y_train_check = [1 if x > 0.5 else 0 for x in y_train]\n",
    "\n",
    "# Print the confusion matrix\n",
    "conf_matrix = confusion_matrix(yhat, y_train_check)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Print the classification report\n",
    "class_report = classification_report(yhat, y_train_check, digits=3)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a2e1e4-dd89-4a33-aed8-18d69783272b",
   "metadata": {},
   "source": [
    "# 2.2.2 [graded question]\n",
    "\n",
    "Compute the confusion matrix and the classifi\f",
    "cation report for your logistic regression model.\n",
    "Interpret the metrics.\n",
    "\n",
    "True Positives (TP): The model correctly predicted that a company did not go bankrupt in 5266 cases.\n",
    "\r\n",
    "True Negatives (TN): he model correctly predicted that a company went bankrupt in 31 cases.\n",
    ".\r\n",
    "False Positives (FP)These are cases where the model incorrectly predicted that a company went bankrupt when it did not.\n",
    ").\r\n",
    "False Negatives (FN)he model incorrectly predicted that a company did not go bankrupt when it did in 15 caseso\n",
    "\n",
    "Accuracy: (TP + TN) / (TP+FN+FP+TN) 5266 + 31 / 5266+15+143+31 = 5297/5455 = 0.9710357470210816\n",
    "The accuracy or the performance of the logistic regression classification model is 0.971. It indicates that the model is accurate 97.1% of the time.\n",
    "\n",
    "Precision: Precision quantifies the number of correct positive predictions made out of positive predictions made by the model. Precision = TP/(TP + FP)\n",
    "Precision for class 0 (Did not go bankrupt): 0.997 - This means that when the model predicted that a company did not go bankrupt, it is correct 99.7% of the time.\r\n",
    "Precision for class 1 Got bankrupt): 0.178 - This means that when the model predicts a company went bankrupt, it is correct 17.8% of the time.\n",
    "\n",
    "Recall (Sensitivity):\n",
    " The ratio of true positives to the total number of actual positives (TP / (TP + FN)). \n",
    "It measures the model's ability to identify all relevant instances.Recall for class 0 (Did not go bankrupt): 0.974 - This means that the model correctly identifies 97.4% of the companies that did not go bankrupt.\r\n",
    "Recall for class 1 Got bankrupt): 0.674 - This means that the model correctly identifies 67.4% of the companies that went bankrupt\n",
    "\n",
    "\n",
    "F1-score: The harmonic mean of precision and recall, providing a balance between the two metrics\n",
    "F1-score for class 0: 0.985 - The F1-score is the harmonic mean of precision and recall for class 0. It provides a balance between precision and recall.\r\n",
    "F1-score for class 1: 0.282 - The F1-score for class 1 is lower, indicating a trade-off between precision and recall.t.\n",
    "\n",
    "Support: The number of actual occurrences of each class in the test seSupport for class 0: 5409 - This is the actual number of companies that did not go bankrupt in the test set.\r\n",
    "Support for class 1: 46 - This is the actual number of companies that went bankrupt in the test set.\n",
    "tInterpretation:\r\n",
    "\r\n",
    "The model demonstrates high precision and recall for class 0 (companies that did not go bankrupt), indicating that it is effective in identifying non-bankrupt companies.\r\n",
    "However, the model has lower precision and recall for class 1 (companies that went bankrupt), suggesting room for improvement in identifying bankrupt companies.\r\n",
    "The F1-score for class 1 is relatively low, indicating that the trade-off between precision and recall for bankrupt companies might not be favorable.\r\n",
    "The macro average metrics provide an unweighted average, while the weighted average metrics consider the class distribution. The weighted averages indicate strong overall performance.\r\n",
    "The high accuracy suggests that the model correctly predicts the majority of instances, but this might be influenced by the class imbal\n",
    "\n",
    "ance.\r\n",
    "In summary, while the model performs well in identifying non-bankrupt companies, it has challenges in correctly identifying bankrupt co.\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07fe4174-dc1b-47b4-9ae0-23e93a0375de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01852332 0.01171795 0.00320048 0.00050652 0.01025083 0.02765124\n",
      " 0.00321971 0.01693749 0.0829795  0.02034399]\n"
     ]
    }
   ],
   "source": [
    "predictions = logreg.predict()\n",
    "print(predictions[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10cef6e6-738d-4f07-a03c-434479605620",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = pd.read_csv(\"x_test.csv\", index_col=0)\n",
    "y_test = pd.read_csv(\"y_test.csv\", index_col=0)\n",
    "x_train = pd.read_csv(\"x_train.csv\", index_col=0)\n",
    "y_train = pd.read_csv(\"y_train.csv\", index_col=0)\n",
    "# Get the estimated probabilities for the test set\n",
    "yhat_test_logreg_probs = logreg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c5039d7-009d-4c61-866b-efe90db0de76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert probabilities to binary class labels using a threshold of 0.5\n",
    "yhat_test = [1 if x > 0.5 else 0 for x in yhat_test_logreg_probs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "412a8a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Test Set:\n",
      "[[1306   39]\n",
      " [  12    7]]\n",
      "\n",
      "Classification Report for Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.991     0.971     0.981      1345\n",
      "           1      0.152     0.368     0.215        19\n",
      "\n",
      "    accuracy                          0.963      1364\n",
      "   macro avg      0.572     0.670     0.598      1364\n",
      "weighted avg      0.979     0.963     0.970      1364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the confusion matrix for the test set\n",
    "conf_matrix_test = confusion_matrix(yhat_test, y_test)\n",
    "print(\"Confusion Matrix for Test Set:\")\n",
    "print(conf_matrix_test)\n",
    "\n",
    "# Print the classification report for the test set\n",
    "class_report_test = classification_report(yhat_test, y_test, digits=3)\n",
    "print(\"\\nClassification Report for Test Set:\")\n",
    "print(class_report_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa41369-d830-4e99-902e-a48ae2f59be1",
   "metadata": {},
   "source": [
    "# 2.2.3. [graded question]\n",
    "\n",
    "Interpret the results. \n",
    "\n",
    "Interpretation:\r\n",
    "\r\n",
    "The model demonstrates high precision and recall for class 0 (companies that did not go bankrupt), indicating that it is effective in identifying non-bankrupt companies.\r\n",
    "However, the model has lower precision and recall for class 1 (companies that went bankrupt), suggesting room for improvement in identifying bankrupt companies.\r\n",
    "The F1-score for class 1 is relatively low, indicating that the trade-off between precision and recall for bankrupt companies might not be favorable.\r\n",
    "The macro average metrics provide an unweighted average, while the weighted average metrics consider the class distribution. The weighted averages indicate strong overall performance.\r\n",
    "The high accuracy suggests that the model correctly predicts the majority of instances, but this might be influenced by the class imbalance.\n",
    "\n",
    "\n",
    "Do you think \n",
    "is it appropriate to consider mainly the overall accuracy in an imbalanced da\n",
    "\n",
    "No, it is not appropriate to consider mainly the overall accuracy in an imbalanced data set. The reason is that accuracy alone can be misleading and does not provide a complete picture of how well a model is performing, especially when one class significantly outnumbers the other.\n",
    "\n",
    "a se \n",
    " If not \r\n",
    "which metrics are more r\n",
    "The choice of relevant metrics depends on the nature of the dataset and the specific goals of the application. In this case, detecting if a company did not go bankrupt or go bankrupt, we want to prioritize precision to minimize false positives since the cost of having false positives is high. We also consider the trade-offs between precision and recall based on the practical implications of false positives and false negatives in our application.elevat ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e078727-fe48-4757-b619-7ef399fcc109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01852332 0.01171795 0.00320048 0.00050652 0.01025083 0.02765124\n",
      " 0.00321971 0.01693749 0.0829795  0.02034399]\n"
     ]
    }
   ],
   "source": [
    "predictions = logreg.predict()\n",
    "print(predictions[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab60f609-f894-4612-bb5b-0396344eb0db",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc7db952-7541-4892-a2e9-3249fd57ba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = scaler_x.fit_transform(x_train)\n",
    "y_train = scaler_y.fit_transform(y_train)\n",
    "\n",
    "x_test = scaler_x.transform(x_test)\n",
    "y_test = scaler_y.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76cc602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDITED\n",
    "# K-NEAREST NEIGHBOUR\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Create a KNN classifier with K = 1\n",
    "neigh = KNeighborsClassifier(n_neighbors=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "438a09c9-2e0e-4847-8d88-fed0a5b9517c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.18151668]\n",
      " [-0.18151668]\n",
      " [-0.18151668]\n",
      " [-0.18151668]\n",
      " [-0.18151668]\n",
      " [-0.18151668]\n",
      " [-0.18151668]\n",
      " [-0.18151668]\n",
      " [-0.18151668]\n",
      " [-0.18151668]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01d46dc2-1ba3-4136-902c-94a4797c6156",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_binary = [1 if x > 0.5 else 0 for x in y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6b0c5f6-bcc2-48f0-9245-50a889b9a43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the KNN model on the training set\n",
    "neigh.fit(x_train,np.ravel(y_train_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96ef8cf6-2afe-4c33-bf38-7a0807221ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "yhat_knn=neigh.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7efe4762-6707-41ab-948b-4968a161cbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(yhat_knn[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be8360ee-cde4-475a-add1-f5937e4cc140",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_binary = [1 if x > 0.5 else 0 for x in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03e84f86-e5bf-4fce-84f0-35d6b41da8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.18151668]\n",
      " [-0.18151668]\n",
      " [-0.18151668]\n",
      " [-0.18151668]\n",
      " [ 5.50913557]\n",
      " [-0.18151668]\n",
      " [-0.18151668]\n",
      " [-0.18151668]\n",
      " [-0.18151668]\n",
      " [-0.18151668]]\n"
     ]
    }
   ],
   "source": [
    "print(y_test[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0159cde5-2ae8-4e59-a848-8b6c1a1c7f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "bal_acc = balanced_accuracy_score(y_test_binary,yhat_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f118c5f-c69e-43e2-9738-a4b26bdbfb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.621709441182292\n"
     ]
    }
   ],
   "source": [
    "print(bal_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "52aa3045-14d4-4512-9b27-2e8fa61cb657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n"
     ]
    }
   ],
   "source": [
    "K = [i for i in range(1, 21)]\n",
    "print(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c1a1a3ff-138e-4560-b693-b46d7228e3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bal_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2913b438-4771-44c0-ad5b-dc9d017b36f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in K:\n",
    "  neigh = KNeighborsClassifier(n_neighbors=k)\n",
    "  neigh.fit(x_train,np.ravel(y_train_binary))\n",
    "  yhat_knn = neigh.predict(x_test)\n",
    "  y_test_binary = [1 if x > 0.5 else 0 for x in y_test]\n",
    "  bal_acc.append(balanced_accuracy_score(y_test_binary,yhat_knn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aee80535-b52e-416a-ba8d-ccfa8f1af0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(yhat_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e5709d5d-ace9-46b7-b00e-95730c54f07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.621709441182292, 0.5310912449693211, 0.5947911855908161, 0.5749488685095996, 0.616909678696312, 0.5846803457148513, 0.6161509533548856, 0.5633205779507818, 0.6165303160255987, 0.5966879989443822, 0.5959292736029558, 0.5850597083855644, 0.5850597083855644, 0.5524510127333905, 0.5741901431681731, 0.5640793032922082, 0.5625618526093554, 0.5633205779507818, 0.5625618526093554, 0.5524510127333905]\n"
     ]
    }
   ],
   "source": [
    "print(bal_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "51adcf6d-49a9-4437-8334-183b12e8ef1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Balanced Accuracy Score: 0.621709441182292\n",
      "Index of Maximum Balance Accuracy Score: 0\n"
     ]
    }
   ],
   "source": [
    "max_value = max(bal_acc)\n",
    "max_index = bal_acc.index(max_value)\n",
    "\n",
    "print(\"Maximum Balanced Accuracy Score:\", max_value)\n",
    "print(\"Index of Maximum Balance Accuracy Score:\", max_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1c716558-2d42-482d-ac11-29f15d0080b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n"
     ]
    }
   ],
   "source": [
    "print(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "771e7200-8d82-4e73-8815-33ad8cbe712a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we choose k=5 having the next highest value of balanced accuracy score \n",
    "k=5\n",
    "neigh = KNeighborsClassifier(n_neighbors=k)\n",
    "neigh.fit(x_train,np.ravel(y_train_binary))\n",
    "yhat_knn=neigh.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e95a88f1-bff4-42c2-aa1a-705693cd4339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1311   35]\n",
      " [   7   11]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(yhat_knn, y_test_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e0ec2c67-604f-4e52-abb0-33a3cc1390b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.995     0.974     0.984      1346\n",
      "           1      0.239     0.611     0.344        18\n",
      "\n",
      "    accuracy                          0.969      1364\n",
      "   macro avg      0.617     0.793     0.664      1364\n",
      "weighted avg      0.985     0.969     0.976      1364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(yhat_knn, y_test_binary,digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b15b118-2b8d-4ada-9077-02fad3b6f9b5",
   "metadata": {},
   "source": [
    "# 2.3.1. [graded question]\n",
    "What value of K would you choose? \n",
    "I would select the value of K with the highest balanced accuracy score which is K=1. However, this would not give an interesting outcome because K=1 indicates that we are only considering the nearest neighbor when making a classification decision and the KNN model is very flexible and can closely fit the training data, potentially capturing errors and outliers. However, this flexibility can lead to overfitting and sensitivity to outliers.\n",
    "\n",
    "Consequently, we select the next highest value of the balanced accuracy score for which K = 5 \n",
    "\n",
    "For this value \n",
    "of K calculate the performance indicators, confusion matrix and classi\f",
    "cation report, on the\r\n",
    "test set. Interpret the result\n",
    "\n",
    "INTERPRETATION\n",
    "The model demonstrates strong performance in correctly identifying companies that did not go bankrupt (class 0). This is reflected in high precision and recall for class 0.\r\n",
    "\r\n",
    "However, there is room for improvement in identifying companies that went bankrupt (class 1). The precision and recall for class 1 are lower, indicating that the model occasionally makes incorrect predictions for bankrupt companies.\r\n",
    "\r\n",
    "The F1-score for class 1 is relatively modest, suggesting that there is a trade-off between precision and recall for bankrupt companies. It implies that improving one of these metrics might come at the expense of the other.\r\n",
    "\r\n",
    "The macro and weighted averages provide a comprehensive view of the model's overall performance, and they show favorable results. However, the primary focus should be on enhancing the model's ability to identify companies that went bankrupt (class 1).\r\n",
    "\r\n",
    "While the model exhibits high overall accuracy, there is an opportunity to fine-tune the model. Adjusting the prediction threshold or exploring different values of K in the K-Nearest Neighbors (KNN) algorithm may help strike a better balance between precision and recall, particularly for the minority class (clss 1).\n",
    "\n",
    "s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ed55287e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Discriminant Analysis (LDA):\n",
      "Confusion Matrix:\n",
      "[[1299   19]\n",
      " [  31   15]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.977     0.986     0.981      1318\n",
      "           1      0.441     0.326     0.375        46\n",
      "\n",
      "    accuracy                          0.963      1364\n",
      "   macro avg      0.709     0.656     0.678      1364\n",
      "weighted avg      0.959     0.963     0.961      1364\n",
      "\n",
      "\n",
      "Quadratic Discriminant Analysis (QDA):\n",
      "Confusion Matrix:\n",
      "[[1280   38]\n",
      " [  30   16]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.977     0.971     0.974      1318\n",
      "           1      0.296     0.348     0.320        46\n",
      "\n",
      "    accuracy                          0.950      1364\n",
      "   macro avg      0.637     0.659     0.647      1364\n",
      "weighted avg      0.954     0.950     0.952      1364\n",
      "\n",
      "[0.96810266 0.03189734]\n",
      "[[5.07499371e-01 5.61487295e-01 5.56092411e-01 1.17719099e-01\n",
      "  5.55001102e+09 1.11500340e-01 8.15420501e-01 1.25669967e-01\n",
      "  8.96077591e-02 5.93879112e-01 3.06814657e-02 8.09838677e-01]\n",
      " [4.21282047e-01 4.58457712e-01 4.65005880e-01 3.33512513e-02\n",
      "  4.96788333e+09 1.85968720e-01 7.49489244e-01 4.96204862e-02\n",
      "  1.39421993e-01 5.58209193e-01 6.24193244e-02 7.39513902e-01]]\n"
     ]
    }
   ],
   "source": [
    "# DISCRIMINANT ANALYSIS\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "x_test = pd.read_csv(\"x_test.csv\", index_col=0)\n",
    "y_test = pd.read_csv(\"y_test.csv\", index_col=0)\n",
    "x_train = pd.read_csv(\"x_train.csv\", index_col=0)\n",
    "y_train = pd.read_csv(\"y_train.csv\", index_col=0)\n",
    "\n",
    "# Linear Discriminant Analysis (LDA)\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "lda_model.fit(x_train, np.ravel(y_train))\n",
    "yhat_lda = lda_model.predict(x_test)\n",
    "\n",
    "# Quadratic Discriminant Analysis (QDA)\n",
    "qda_model = QuadraticDiscriminantAnalysis()\n",
    "qda_model.fit(x_train, np.ravel(y_train))\n",
    "yhat_qda = qda_model.predict(x_test)\n",
    "\n",
    "# Evaluate LDA\n",
    "conf_matrix_lda = confusion_matrix(y_test, yhat_lda)\n",
    "class_report_lda = classification_report(y_test, yhat_lda, digits=3)\n",
    "\n",
    "# Evaluate QDA\n",
    "conf_matrix_qda = confusion_matrix(y_test, yhat_qda)\n",
    "class_report_qda = classification_report(y_test, yhat_qda, digits=3)\n",
    "\n",
    "# Print the results\n",
    "print('Linear Discriminant Analysis (LDA):')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix_lda)\n",
    "print('Classification Report:')\n",
    "print(class_report_lda)\n",
    "\n",
    "print('\\nQuadratic Discriminant Analysis (QDA):')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix_qda)\n",
    "print('Classification Report:')\n",
    "print(class_report_qda)\n",
    "\n",
    "print(lda_model.priors_)\n",
    "print(lda_model.means_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dba2b2-332d-4dfa-bc23-e3a3849c7788",
   "metadata": {},
   "source": [
    "# 2.4.1 [graded question] Interpret the prior probabilities as well as the group means.\r\n",
    "Lda_model.priors_ : This array contains the prior probabilities of the classes in LDA which in this case are [0.96810266  0.03189734].\n",
    " This simply means that the prior probabilities for class 0 and class 1 are 0.96810266 and 0.03189734.\r\n",
    "Lda_model.means_: This array contains the means of each feature for each class. The rows correspond to the classes and the columns correspond to the features. The result for our given data  the code result aboves:02e-01]]\r\n",
    "\r\n",
    "This information is useful for understanding the distribution of the data within each class and the assumed class priors. It can provide insight into the model’s decision-making process.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df899c4-3e1b-4dca-b7ba-590e2bfb76a6",
   "metadata": {},
   "source": [
    "# 2.4.2. [graded question] Calculate the confusion matrix and the classi\f",
    "cation report on the test set\r\n",
    "for the model obtained with the LDA classi\f",
    "er. Discuss about the obtained metrics (speci\f",
    "city,\r\n",
    "sensitivity,precision, f1-score, etc.\n",
    "\n",
    "Metrics Interpretation:\r\n",
    "Accuracy: Accuracy measures the overall correctness of the model.\r\n",
    "Formula: (TP + TN) / (TP + TN + FP + FN)\r\n",
    "Value: 0.963 or 96.3%\r\n",
    "Precision (Positive Predictive Value): Precision measures the accuracy of the positive predictions.\r\n",
    "Formula: TP / (TP + FP)\r\n",
    "Value: 0.441 or 44.1% for class 1.\r\n",
    "Recall (Sensitivity or True Positive Rate): Recall measures the proportion of actual positives that were correctly predicted. \r\n",
    "Formula: TP / (TP + FN)\r\n",
    "Value: 0.326 or 32.6% for class 1.\r\n",
    "F1-Score: F1-Score is the harmonic mean of precision and recall.\r\n",
    "Formula: 2 * (Precision * Recall) / (Precision + Recall)\r\n",
    "Value: 0.375 or 37.5% for class 1.\r\n",
    " Interpretation:\r\n",
    "The model has high accuracy (96.3%), but accuracy can be misleading in imbalanced datasets.\r\n",
    "Precision for class 1 is relatively low (44.1%), indicating a significant number of false positives.\r\n",
    "Recall for class 1 is also relatively low (32.6%), indicating that the model misses a significant number of actual positive instances.\r\n",
    "The F1-score, which balances precision and recall, is also low for \n",
    "class 1 (37.5%).\r\n",
    "Conclusion:\r\n",
    "The model performs well in terms of overall accuracy but struggles to correctly identify instances of class 1, as evidenced by low precision, recall, and F1-score for that class. Depending on the specific use case and the cost associated with false positives and false negatives, further model tuning or exploring other algorithms may be necessary.\r\n",
    ")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df0a102-d41c-4938-8ebe-88cd988e5eb7",
   "metadata": {},
   "source": [
    "# 2.4.3\r\n",
    "The provided result is based on the evaluation of a Quadratic Discriminant Analysis (QDA) model. Let's discuss the obtained metrics:\r\n",
    " Confusion Matrix:\r\n",
    "[[1280   38]\r\n",
    " [ 30   16]\r\n",
    "True Positive (TP): 16; True Negative (TN): 1280; False Positive (FP): 38; False Negative (FN): 30\r\n",
    "\r\n",
    "Classification Report:\r\n",
    "`\r\n",
    "              precision    recall  f1-score   support\r\n",
    "\r\n",
    "           0      0.977     0.971     0.974      1318\r\n",
    "           1      0.296     0.348     0.320        46\r\n",
    "\r\n",
    "    accuracy                          0.950      1364\r\n",
    "   macro avg      0.637     0.659     0.647      1364\r\n",
    "weighted avg      0.954     0.950     0.952      1364\r\n",
    "\r\n",
    "Metrics Interpretation:\r\n",
    "Accuracy:\r\n",
    "Formula: (TP + TN) / (TP + TN + FP + FN)\r\n",
    "Value: 0.950 or 95.0%\r\n",
    "Precision (Positive Predictive Value):\r\n",
    "Formula: TP / (TP + FP)\r\n",
    "Value: 0.296 or 29.6% for class 1.\r\n",
    "Recall (Sensitivity or True Positive Rate):\r\n",
    "Formula: TP / (TP + FN)\r\n",
    "Value: 0.348 or 34.8% for class 1.\r\n",
    "F1-Score:\r\n",
    "Formula: 2 * (Precision * Recall) / (Precision + Recall)\r\n",
    "Value: 0.320 or 32.0% for class 1.\r\n",
    "Interpretation:\r\n",
    "The model has a high overall accuracy (95.0%). Precision for class 1 is relatively low (29.6%), indicating a significant number of false positives. Recall for class 1 is also relatively low (34.8%), indicating that the model misses a significant number of actual positive instances. The F1-score for class 1 is also low (32.0%).\r\n",
    "Conclusion:\r\n",
    "Similar to the LDA model, the QDA model struggles to correctly identify instances of class 1, as evidenced by low precision, recall, and F1-score for that class. Further model tuning or exploring other algorithms may be necessary to improve performance, especially if the specific use case has a preference or better identification of positive instances.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "345733fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: inf, FDR: 1.0000, TPR: 0.0000\n",
      "Threshold: 1.0000, FDR: 1.0000, TPR: 0.0000\n",
      "Threshold: 1.0000, FDR: 0.9565, TPR: 0.0435\n",
      "Threshold: 1.0000, FDR: 0.9565, TPR: 0.0435\n",
      "Threshold: 0.9981, FDR: 0.9130, TPR: 0.0870\n",
      "Threshold: 0.9891, FDR: 0.9130, TPR: 0.0870\n",
      "Threshold: 0.9877, FDR: 0.8913, TPR: 0.1087\n",
      "Threshold: 0.9864, FDR: 0.8913, TPR: 0.1087\n",
      "Threshold: 0.9803, FDR: 0.8696, TPR: 0.1304\n",
      "Threshold: 0.9420, FDR: 0.8696, TPR: 0.1304\n",
      "Threshold: 0.9264, FDR: 0.8478, TPR: 0.1522\n",
      "Threshold: 0.9139, FDR: 0.8478, TPR: 0.1522\n",
      "Threshold: 0.9045, FDR: 0.8261, TPR: 0.1739\n",
      "Threshold: 0.8955, FDR: 0.8261, TPR: 0.1739\n",
      "Threshold: 0.8637, FDR: 0.7826, TPR: 0.2174\n",
      "Threshold: 0.8378, FDR: 0.7826, TPR: 0.2174\n",
      "Threshold: 0.8230, FDR: 0.7609, TPR: 0.2391\n",
      "Threshold: 0.7654, FDR: 0.7609, TPR: 0.2391\n",
      "Threshold: 0.7523, FDR: 0.7391, TPR: 0.2609\n",
      "Threshold: 0.6555, FDR: 0.7391, TPR: 0.2609\n",
      "Threshold: 0.6175, FDR: 0.6957, TPR: 0.3043\n",
      "Threshold: 0.5933, FDR: 0.6957, TPR: 0.3043\n",
      "Threshold: 0.5170, FDR: 0.6739, TPR: 0.3261\n",
      "Threshold: 0.4212, FDR: 0.6739, TPR: 0.3261\n",
      "Threshold: 0.3481, FDR: 0.6522, TPR: 0.3478\n",
      "Threshold: 0.3179, FDR: 0.6522, TPR: 0.3478\n",
      "Threshold: 0.3158, FDR: 0.6304, TPR: 0.3696\n",
      "Threshold: 0.2766, FDR: 0.6304, TPR: 0.3696\n",
      "Threshold: 0.2535, FDR: 0.5435, TPR: 0.4565\n",
      "Threshold: 0.1564, FDR: 0.5435, TPR: 0.4565\n",
      "Threshold: 0.1401, FDR: 0.5217, TPR: 0.4783\n",
      "Threshold: 0.1375, FDR: 0.5217, TPR: 0.4783\n",
      "Threshold: 0.1239, FDR: 0.5000, TPR: 0.5000\n",
      "Threshold: 0.0965, FDR: 0.5000, TPR: 0.5000\n",
      "Threshold: 0.0946, FDR: 0.4783, TPR: 0.5217\n",
      "Threshold: 0.0877, FDR: 0.4783, TPR: 0.5217\n",
      "Threshold: 0.0865, FDR: 0.4565, TPR: 0.5435\n",
      "Threshold: 0.0860, FDR: 0.4565, TPR: 0.5435\n",
      "Threshold: 0.0829, FDR: 0.4348, TPR: 0.5652\n",
      "Threshold: 0.0735, FDR: 0.4348, TPR: 0.5652\n",
      "Threshold: 0.0729, FDR: 0.4130, TPR: 0.5870\n",
      "Threshold: 0.0711, FDR: 0.4130, TPR: 0.5870\n",
      "Threshold: 0.0703, FDR: 0.3913, TPR: 0.6087\n",
      "Threshold: 0.0586, FDR: 0.3913, TPR: 0.6087\n",
      "Threshold: 0.0586, FDR: 0.3696, TPR: 0.6304\n",
      "Threshold: 0.0534, FDR: 0.3696, TPR: 0.6304\n",
      "Threshold: 0.0506, FDR: 0.3478, TPR: 0.6522\n",
      "Threshold: 0.0473, FDR: 0.3478, TPR: 0.6522\n",
      "Threshold: 0.0459, FDR: 0.3261, TPR: 0.6739\n",
      "Threshold: 0.0401, FDR: 0.3261, TPR: 0.6739\n",
      "Threshold: 0.0396, FDR: 0.3043, TPR: 0.6957\n",
      "Threshold: 0.0386, FDR: 0.3043, TPR: 0.6957\n",
      "Threshold: 0.0363, FDR: 0.2826, TPR: 0.7174\n",
      "Threshold: 0.0346, FDR: 0.2826, TPR: 0.7174\n",
      "Threshold: 0.0340, FDR: 0.2609, TPR: 0.7391\n",
      "Threshold: 0.0339, FDR: 0.2609, TPR: 0.7391\n",
      "Threshold: 0.0329, FDR: 0.2174, TPR: 0.7826\n",
      "Threshold: 0.0167, FDR: 0.2174, TPR: 0.7826\n",
      "Threshold: 0.0165, FDR: 0.1957, TPR: 0.8043\n",
      "Threshold: 0.0141, FDR: 0.1957, TPR: 0.8043\n",
      "Threshold: 0.0140, FDR: 0.1739, TPR: 0.8261\n",
      "Threshold: 0.0138, FDR: 0.1739, TPR: 0.8261\n",
      "Threshold: 0.0138, FDR: 0.1522, TPR: 0.8478\n",
      "Threshold: 0.0105, FDR: 0.1522, TPR: 0.8478\n",
      "Threshold: 0.0104, FDR: 0.1304, TPR: 0.8696\n",
      "Threshold: 0.0068, FDR: 0.1304, TPR: 0.8696\n",
      "Threshold: 0.0067, FDR: 0.1087, TPR: 0.8913\n",
      "Threshold: 0.0066, FDR: 0.1087, TPR: 0.8913\n",
      "Threshold: 0.0066, FDR: 0.0870, TPR: 0.9130\n",
      "Threshold: 0.0045, FDR: 0.0870, TPR: 0.9130\n",
      "Threshold: 0.0044, FDR: 0.0652, TPR: 0.9348\n",
      "Threshold: 0.0043, FDR: 0.0652, TPR: 0.9348\n",
      "Threshold: 0.0043, FDR: 0.0435, TPR: 0.9565\n",
      "Threshold: 0.0032, FDR: 0.0435, TPR: 0.9565\n",
      "Threshold: 0.0032, FDR: 0.0217, TPR: 0.9783\n",
      "Threshold: 0.0017, FDR: 0.0217, TPR: 0.9783\n",
      "Threshold: 0.0016, FDR: 0.0000, TPR: 1.0000\n",
      "Threshold: 0.0000, FDR: 0.0000, TPR: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area Under the Curve (AUC): 0.93\n"
     ]
    }
   ],
   "source": [
    "# ROC (Receiver Operating Characteristic) Curve\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Having lda_model as our trained Linear Discriminant Analysis (LDA) model\n",
    "lda_scores = lda_model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, lda_scores)\n",
    "\n",
    "# Calculate AUC\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "#ADDED CODE\n",
    "# Calculate False Discovery Rate (FDR)\n",
    "fdr = 1 - tpr\n",
    "\n",
    "# Print FDR and TPR\n",
    "for i, threshold in enumerate(thresholds):\n",
    "    print(f\"Threshold: {threshold:.4f}, FDR: {fdr[i]:.4f}, TPR: {tpr[i]:.4f}\")\n",
    "    #END OF ADD\n",
    "\n",
    "# Plot ROC curve (optional)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Display AUC\n",
    "print('Area Under the Curve (AUC): {:.2f}'.format(roc_auc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba966060-e87a-4919-8230-395439c3d9d8",
   "metadata": {},
   "source": [
    "# 2.5.1 interpret the outputs of the roc_curve() function, namely fpr and tpr.\r",
    " Here's an interpretation of the outputs:\r\n",
    "Threshold: The threshold used to classify the instances into positive or negative classes.\r\n",
    "FDR (False Discovery Rate): The proportion of predicted positive instances that are negative. It is calculated as False Positives / (False Positives + True Positives).\r\n",
    "TPR (True Positive Rate): Also known as Sensitivity or Recall, it is the proportion of actual positive instances that are correctly predicted as positive. It is calculated as True Positives / (True Positives + False Negativ\n",
    "es).\r\n",
    "Observations:\r\n",
    "Threshold Selection: The threshold decreases from the top to the bottom, which means the model becomes more permissive in classifying instances as positive.\r\n",
    "Trade-off Between FDR and TPR: As the threshold decreases, FDR tends to decrease, and TPR tends to increase. This indicates a trade-off between precision (FDR) and recall (TPR). Lowering the threshold results in capturing more true positives but may also increase false positives.\r\n",
    "Imbalance Impact: The class imbalance is evident in the fact that even at very high thresholds, TPR is not zero, indicating that the model is still able to capture some positive instances.\r\n",
    "Optimal Threshold: The optimal threshold depends on the specific goals of your model. If you want to prioritize minimizing false positives, you might choose a higher threshold. If capturing as many positives as possible is crucial, you might choose a lower threshold.\r\n",
    "Overall, understanding this trade-off is crucial, and the choice of the threshold should align with the application and the importance of precision and recall in the specific context.\r\n",
    "Since the AUC (0.93) is closer to 1, this indicates better discriminatory power. \r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1216d643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC for LDA: 0.9251995777528534\n",
      "AUC for QDA: 0.8813831892854787\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'logreg_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 26\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAUC for QDA:\u001b[39m\u001b[38;5;124m'\u001b[39m, roc_auc_qda)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Having logreg_model as our trained logistic regression model from statsmodels\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Get the predicted probabilities\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m logit_probs \u001b[38;5;241m=\u001b[39m \u001b[43mlogreg_model\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(x_test)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Calculate ROC curve\u001b[39;00m\n\u001b[0;32m     29\u001b[0m fpr_logit, tpr_logit, thresholds_logit \u001b[38;5;241m=\u001b[39m roc_curve(y_test, logit_probs)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'logreg_model' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Having lda_model as our trained Linear Discriminant Analysis (LDA) model\n",
    "lda_scores = lda_model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr_lda, tpr_lda, thresholds_lda = roc_curve(y_test, lda_scores)\n",
    "\n",
    "# Calculate AUC\n",
    "roc_auc_lda = auc(fpr_lda, tpr_lda)\n",
    "print('AUC for LDA:', roc_auc_lda)\n",
    "\n",
    "# Having qda_model as our trained Quadratic Discriminant Analysis (QDA) model\n",
    "qda_scores = qda_model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr_qda, tpr_qda, thresholds_qda = roc_curve(y_test, qda_scores)\n",
    "\n",
    "# Calculate AUC\n",
    "roc_auc_qda = auc(fpr_qda, tpr_qda)\n",
    "print('AUC for QDA:', roc_auc_qda)\n",
    "\n",
    "# Having logreg_model as our trained logistic regression model from statsmodels\n",
    "\n",
    "# Get the predicted probabilities\n",
    "logit_probs = logreg_model.predict(x_test)\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr_logit, tpr_logit, thresholds_logit = roc_curve(y_test, logit_probs)\n",
    "\n",
    "# Calculate AUC\n",
    "roc_auc_logit = auc(fpr_logit, tpr_logit)\n",
    "print('AUC for Logistic Regression (statsmodels):', roc_auc_logit)\n",
    "\n",
    "# Having neigh as our trained KNN model with the chosen value of K\n",
    "knn_scores = neigh.predict_proba(x_test)[:, 1]\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr_knn, tpr_knn, thresholds_knn = roc_curve(y_test, knn_scores)\n",
    "\n",
    "# Calculate AUC\n",
    "roc_auc_knn = auc(fpr_knn, tpr_knn)\n",
    "print('AUC for KNN:', roc_auc_knn)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91be85f-ba66-4ff5-96e6-8af34d13979d",
   "metadata": {},
   "source": [
    "# 2.5.2\n",
    "Calculate the auc and plot the ROC curve for the 4 models LDA, QDA,\n",
    "logistic regression and KNN with the chosen value of K. According to the ROC curve which\n",
    "model would you choose?\n",
    "\n",
    "\n",
    "The AUC values for the 4 models are:\n",
    "AUC for LDA: 0.9251995777528534\n",
    "AUC for QDA: 0.8813831892854787\n",
    "AUC for Logistic Regression (statsmodels): 0.9381144025862638\n",
    "AUC for KNN: 0.7075938510259286\n",
    "\n",
    "The model with the highest AUC value performs better in terms of the trade-off between sensitivity and specificity. A higher AUC generally indicates a better-performing model. Given our computed AUC values, the LDA model would be the ideal choice.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f839e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Having lda_model as our trained Linear Discriminant Analysis (LDA) model\n",
    "lda_scores = lda_model.predict_proba(x_test)[:, 1]\n",
    "fpr_lda, tpr_lda, thresholds_lda = roc_curve(y_test, lda_scores)\n",
    "roc_auc_lda = auc(fpr_lda, tpr_lda)\n",
    "\n",
    "# Having qda_model as our trained Quadratic Discriminant Analysis (QDA) model\n",
    "qda_scores = qda_model.predict_proba(x_test)[:, 1]\n",
    "fpr_qda, tpr_qda, thresholds_qda = roc_curve(y_test, qda_scores)\n",
    "roc_auc_qda = auc(fpr_qda, tpr_qda)\n",
    "\n",
    "# Having logreg_model as our trained logistic regression model from scikit-learn\n",
    "logreg_model = LogisticRegression()\n",
    "logreg_model.fit(x_train, y_train)\n",
    "logreg_scores = logreg_model.predict_proba(x_test)[:, 1]\n",
    "fpr_logreg, tpr_logreg, thresholds_logreg = roc_curve(y_test, logreg_scores)\n",
    "roc_auc_logreg = auc(fpr_logreg, tpr_logreg)\n",
    "\n",
    "# Having neigh as our trained KNN model with the chosen value of K\n",
    "knn_scores = neigh.predict_proba(x_test)[:, 1]\n",
    "fpr_knn, tpr_knn, thresholds_knn = roc_curve(y_test, knn_scores)\n",
    "roc_auc_knn = auc(fpr_knn, tpr_knn)\n",
    "\n",
    "# Plot ROC curves for all models\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_lda, tpr_lda, color='darkorange', lw=2, label='ROC curve for LDA (AUC = {:.2f})'.format(roc_auc_lda))\n",
    "plt.plot(fpr_qda, tpr_qda, color='green', lw=2, label='ROC curve for QDA (AUC = {:.2f})'.format(roc_auc_qda))\n",
    "plt.plot(fpr_logreg, tpr_logreg, color='blue', lw=2, label='ROC curve for Logistic Regression (AUC = {:.2f})'.format(roc_auc_logreg))\n",
    "plt.plot(fpr_knn, tpr_knn, color='purple', lw=2, label='ROC curve for KNN (AUC = {:.2f})'.format(roc_auc_knn))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Diagonal line (random classifier)\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve for Multiple Models')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4960ec-f75a-438e-96f0-232556cf2013",
   "metadata": {},
   "source": [
    "# 2.5.3\r\n",
    " In the ROC curve plot you provided, each curve represents the trade-off between true positive rate (sensitivity) and false positive rate (1-specificity) for different classification thresholds. The area under the ROC curve (AUC) is a measure of the model's ability to discriminate between the positive and negative classes. The larger the AUC, the model with the best performance is Linear Discriminant Analysis with an AUC value of 0.93\n",
    " .\r\n",
    "Now, let's discuss the ROC curves for each model:\r\n",
    "\r\n",
    "Linear Discriminant Analysis (LDA):\r\n",
    "AUC: The curve for LDA (dark orange) indicates good discrimination, and the AUC is approximately 0.93. This suggests that LDA is effective in distinguishing between the two classes.\r\n",
    "Quadratic Discriminant Analysis (QDA):\r\n",
    " AUC: The curve for QDA (green) has an AUC of around 0.88, indicating reasonable discrimination.\r\n",
    "Logistic Regression:\r\n",
    "AUC: The curve for logistic regression (blue) has an AUC of about 0.69, similar to LDA. Logistic regression is also effective in this context.\r\n",
    "K-Nearest Neighbos (KNN):\r\n",
    " AUC: The curve for KNN (purple) has an AUC of approximately 0.71. It performs reasonably well, but not as well as LDA or logistic regression.\r\n",
    "Conclusion:\r\n",
    "The ROC curves provide insights into the models' discrimination abilities, and the AUC values quantify this performance. LDA and logistic regression appear to be the most effective models in this scenario, based on the AUC values. The choice of the optimal classification threshold depends on the specific balance of sensitivity and specificity desired for the\n",
    "\n",
    " application.\r\n",
    "Advantages and Disadvantages of Dealing with Imbalanced Da\n",
    "ta\r\n",
    "Advantages:\r\n",
    "Flexibility: Adjusting the classification threshold allows you to tailor the model's behavior to meet specific requirements.\r\n",
    "Better Handling of Imbalance: By choosing an appropriate threshold, you can mitigate the impact of\n",
    " class imbalance.\r\n",
    "Disadvantages:\r\n",
    "Subjectivity: Selecting a threshold involves subjective decisions and depends on the application's context.\r\n",
    "Trade-offs: Adjusting the threshold often involves trade-offs between sensitivi\n",
    "\n",
    "ty and specificity.\r\n",
    "Recommendation:\r\n",
    "Depending on the application, you might choose a threshold that balances the costs of false positives and false negatives. The optimal threshold could be where the ROC curve intersects a line of equal misclassification cost or based on specific business considerations.\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
